# -*- coding: utf-8 -*-
"""TEXT_CLASSIFICATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLlFoVF3ZeSYwH3_YDGEHLzuqpy5KHSV

**TEXT CLASSIFICATION WITH TENSORFLOW**
  we take a piece of text (sentence,paragraph,review,tweet,etc) and put into different categories ex - positive or negative
  
  Tensorflow-it's a machine learning library by google to train model and helps in recogonize model
  (HERE -->automatically categorize text into predefined groups)
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb

#load the datset (train and test )
(train_data,train_label),(test_data,test_label)=imdb.load_data(num_words=10000)
print(f"training data shape:{train_data.shape}")
print(f"test data shape:{test_data.shape}")

#num_words =10000 means we limit the vocabulary to the top 10k most frequency words in the dataset

"""**PREPROCESSING THE DATA **
there are variuos way sone of the way is padding to make every dat ais of same length ,we will also convert these sequences into vector so that model can understand it  
"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
maxlen=500# we ;imit the length of each review up to 500 words
train_data=pad_sequences(train_data,maxlen=maxlen)
test_data=pad_sequences(test_data,maxlen=maxlen)

print(f"trianing data shape after padding : {train_data.shape}")

"""Padding: This step ensures all reviews are the same length (500 words). Reviews shorter than 500 words are padded with zeros.

**BUILD the MODEL**
it s an interesting part  building the neural network no no just kidding but get an idea by like builiding a neural network


*   **Embedding Layers**: turn word into vectors of fixed size ,forming words into number that a machine can understand
*   **LSTM Layer**: Long short Term Memory ,its great for sequence data ,ecause it keeps track of important information in sequences


*   **Dense Layer**:its the output layer that will classify the reviews as +ve or -ve
"""

model=tf.keras.Sequential([
    tf.keras.layers.Embedding(10000,32,input_length=maxlen),# word embedding
    tf.keras.layers.LSTM(32),#LSTM layer
    tf.keras.layers.Dense(1,activation='sigmoid')# output layer binary clssification
])

#compile the model
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#sumary() of model
model.summary()

"""Embedding(10000, 32): We have 10,000 words in the vocabulary, and each word will be represented as a 32-dimensional vector.

LSTM(32): The LSTM layer has 32 units. This is where the model learns patterns in the sequences of words.

Dense(1, activation='sigmoid'): The output layer has 1 unit (since we’re doing binary classification), and sigmoid is the activation function because we are classifying text as either positive or negative.

**Train The MOdel**
"""

# Train the model
history = model.fit(train_data, train_label, epochs=10, batch_size=60, validation_data=(test_data, test_label))

"""Explanation:

We’re training for 5 or 10 epochs, which means the model will look at the data 5 o 10 times.

Batch size: We set the batch size to 64, meaning the model will look at 64 reviews at a time before adjusting its weights.

**EVALUATE The Model**
"""

test_loss,test_acc=model.evaluate(test_data,test_label)
print(f"Test accuracy: {test_acc}")

"""**Interference(Prediction)**

"""

# Making a prediction
sample_review = ["This movie was amazing!"]
sample_review_seq = imdb.get_word_index()  # Get word-index mapping for IMDB dataset
sample_review = [sample_review_seq.get(word, 0) for word in sample_review[0].lower().split()]
sample_review = pad_sequences([sample_review], maxlen=maxlen)

# Predict the sentiment
prediction = model.predict(sample_review)
print(f"Prediction: {'Positive' if prediction > 0.5 else 'Negative'}")

"""**Wrappimg up Visualizing Axxuracy **"""

import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()
